{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to decrease the learning rate of pytorch optimzier ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Assuming you already have your model, loss function, and optimizer defined\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ...\n",
    "\n",
    "# Create the ReduceLROnPlateau scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train the model\n",
    "    # ...\n",
    "\n",
    "    # Calculate the model performance (accuracy, loss, etc.)\n",
    "    # For example:\n",
    "    current_accuracy = 0.85  # Replace this with your actual model evaluation code.\n",
    "\n",
    "    # Update the learning rate scheduler based on the current model performance\n",
    "    scheduler.step(current_accuracy)\n",
    "\n",
    "    # Check if the model performance improved and save the best model\n",
    "    if current_accuracy > best_accuracy:\n",
    "        best_accuracy = current_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Accuracy: {current_accuracy:.4f}, Best Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# After training, load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "In this example, the learning rate will be reduced by a factor of 0.5 if the model's accuracy does not improve for 3 consecutive epochs (patience). The mode argument can be set to 'min' if you're using a loss function that you want to minimize instead of maximizing accuracy.\n",
    "\n",
    "By using the \"ReduceLROnPlateau\" scheduler, you can dynamically adjust the learning rate based on the model's performance, which often leads to better convergence and more stable training.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How read xlsx file with pandas ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.xlsx' with the path to your XLSX file\n",
    "file_path = 'your_file.xlsx'\n",
    "\n",
    "# Read the XLSX file into a DataFrame\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Now, you can work with the DataFrame 'df' as needed\n",
    "print(df.head())  # Display the first few rows of the DataFrame\n",
    "\n",
    "# Read a specific sheet named 'Sheet2' into a DataFrame\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalance dataset\n",
    "\n",
    "Data Augmentation: Augmenting the minority class (TB) by creating additional synthetic samples can help balance the dataset. Techniques like rotation, flipping, translation, and adding noise can be applied to the TB samples to generate more diverse examples.\n",
    "\n",
    "Data Undersampling: Randomly removing samples from the majority class (normal) to reduce its dominance in the dataset. Undersampling can be effective when you have a large number of samples in the majority class and you're concerned about the computational overhead of working with an imbalanced dataset.\n",
    "\n",
    "Data Oversampling: Duplicating or generating new samples for the minority class (TB) to increase its representation. You can use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic samples based on existing ones.\n",
    "\n",
    "Class Weighting: Most machine learning algorithms and libraries allow you to assign different weights to each class during training. By giving higher weight to the minority class, the model focuses more on learning from those samples.\n",
    "\n",
    "Generating Prototypes: For the minority class, you can generate prototypes using clustering algorithms or other techniques, which represent characteristic patterns of the class and then use them in the training process.\n",
    "\n",
    "Using Different Evaluation Metrics: Accuracy is not always the best metric for imbalanced datasets. Instead, consider using evaluation metrics like precision, recall, F1-score, or area under the receiver operating characteristic curve (AUC-ROC), which give better insights into model performance with imbalanced data.\n",
    "\n",
    "Ensemble Methods: Using ensemble methods like Random Forest, Gradient Boosting, or XGBoost can also help in dealing with imbalanced datasets, as they can handle class imbalance better than some other algorithms.\n",
    "\n",
    "Model Selection: Experiment with different algorithms and architectures to see which ones perform better on imbalanced data. Some models, like SVM and decision trees, can handle imbalanced datasets well.\n",
    "\n",
    "Combine Techniques: Often, the best results are achieved by combining multiple strategies. For example, you can apply data augmentation, oversampling, and class weighting together to improve performance.\n",
    "\n",
    "Transfer Learning: Consider using transfer learning, where you leverage pre-trained models on larger datasets to fine-tune them on your imbalanced dataset. Pre-trained models have learned general features from vast amounts of data and can potentially perform better even with limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weighting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you have defined your model, dataloaders, and other necessary components\n",
    "device = ...\n",
    "num_epochs = ...\n",
    "train_dataloader = ...\n",
    "# Calculate class weights based on the frequency of samples in each class\n",
    "class_weights = torch.tensor([1.0, 3500/700])  # Weight of 'normal' class is 1.0, weight of 'TB' class is (3500/700)\n",
    "\n",
    "# Convert the class weights to device (CPU or GPU)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Define the loss function with class weighting\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Rest of the training loop remains the same\n",
    "for epoch in range(num_epochs):\n",
    "    # Training process\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where dropout should be\n",
    "\n",
    "In general, the dropout layer is usually placed before the ReLU activation layer. The typical order of operations in a neural network layer is as follows:\n",
    "\n",
    "Linear transformation (e.g., a fully connected layer or a convolutional layer)\n",
    "Dropout layer\n",
    "Activation function (e.g., ReLU)\n",
    "The dropout layer is a regularization technique that helps prevent overfitting by randomly setting a fraction of the input units to 0 during training. By placing the dropout layer before the ReLU activation, we allow the dropout to apply to the raw input values before they are passed through the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design patterns to use\n",
    "\n",
    "### 1. Builder pattern\n",
    "### 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
